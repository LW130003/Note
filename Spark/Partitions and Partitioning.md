# Partitions and Partitioning
## Introduction
Depending on how you look at Spark (programmer, devop, admin), an RDD is about the content (developer's and data scientist's perspective) or how it gets spread out over a cluster (performance), i.e. how many partitions an RDD represents.

A **partition** (aka *split*) is a logical chunk of a large distributed data set.

Spark manages data using partitions that helps parallelize distributed data processing with minimal network traffic for sending data between executors.

By default, Spark tries to read data into an RDD from the nodes that are close to it. Since Spark usually accesses distributed partitioned data, to optimize transformation operations it creates partitions to hold the data chunks.

There is a one-to-one correspondence between how data is laid out in data storage like HDFS or Cassandra (it is partitioned for the same reasons).

Features:
- size
- nunber
- partitioning scheme
- node distribution
- repartitioning

By default, a partition is created for each HDFS partition, which by default is 64 MB.

RDDs get partitioned automatically without programmers intervention. However, there are times when you'd like to adjust the size and number of partitions or the partitioning scheme according to the needs of your application.

## Request for the minimum number of partitions, using the second input parameter to many transformation
```scala
sc.parallelize(1 to 100, 2).count
```

## Check number of partitions using **partitions** method of a RDD
```scala
val ints = sc.parallelize(1 to 100, 4)
ints.partitions.size
```

In general, smaller/more numerous partitions allow work to be distributed among more workers, but larger/viewer partitions allow work to be done in large chunks, which may result in the work getting done more quickly as long as all workers are kept busy, due to reduced overhead.

Increasing partitions count will make each partition to have less data (or not at all!)

Spark can only run 1 concurrent task for every partition of an RDD, up to the number of cores in your cluster. So if you have a cluster with 50 cores, you want your RDDs to at least have 50 partitions (and probably 2-3x times that).

As far as choosing a "good" number of partitions, you generally want at least as many as the number of executors for parallelism. You can get this computed by calling
**sc.defaultParallelism**.

Also, the number of partitions determine how many files get generated by actions that save RDDs to files. The maximum size of a partition is ultimately limited by the available memory as an executor.

In the first RDD transformation, e.g. reading from a file using sc.textFile(path, partition), the partition parameter will be applied to all further transformations and actions on this RDD. Partitions get redistributed among nodes whenever **shuffle** occurs. Repartitioning may cause **shuffle** to occur in some situtations, but it is not guaranteed to occur in all cases. And it usually happens during action stage.

When creating an RDD by reading files using rdd = SparkContext().textFile("path") the number of partitions may be smaller. Ideally, you would get the same number of blocks as you see in HDFS, but if the lines in your file are too long (longer than the block size), there will be fewer partitions.

Preferred way to set up the number of partitions for an RDD is to directly pass it as the second input paramter in the call like **rdd = sc.textFile("path", integer)**, where integer is the number of partitions. In this case, the partitioning would be done by Hadoop's TextInputFormat, not Spark and it would work much faster. It will only work as described for uncompressed file.

When using textFile with compressed file (file.text.gz not file.txt or similar), Spark disables splitting that makes for an RDD with only 1 partition (as reads against gzipped files cannot be parallelized). In this case, to change to the number of partitions you should do repartitioning.

Some operations, e.g. map, flatmap, filter, don't preserve partitioning.

map, flatmap, filter operations apply a function to every partition.

